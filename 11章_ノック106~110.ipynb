{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Google Driveと接続を行います。これを行うことで、Driveにあるデータにアクセスできるようになります。\n",
        "# 下記セルを実行すると、Googleアカウントのログインを求められますのでログインしてください。\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EMADV4pYNK-K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4255e9ed-6d75-4eff-9e7e-1f593701c57f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIPfsEyjgiuX"
      },
      "source": [
        "# 11章　深層学習に挑戦する10本ノック"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPclN4iJEC9D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e20ed374-2e0a-48b2-c3e8-97e7b2d5c7e8"
      },
      "source": [
        "# 作業フォルダへの移動を行います。\n",
        "# 人によって作業場所がことなるので、その場合作業場所を変更してください。\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/11章') #ここを変更。\n",
        "\n",
        "!pip install tensorflow\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 放課後ノック105：物体検出YOLOを使って人の検出を行ってみよう"
      ],
      "metadata": {
        "id": "UpmS7--w4YZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### YOLOの準備"
      ],
      "metadata": {
        "id": "Eb7OfRtBvKUC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAKvqj2XnQj-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34607cdc-1ffa-4f16-e9b4-789f0b4612a3"
      },
      "source": [
        "#yolov3-tf2のダウンロード\n",
        "\n",
        "''' GitHubから「yolov3-tf2」というフォルダ（ディレクトリ）を丸ごとコピーして、\n",
        "./yolov3_tf2という名前の新しいフォルダに保存します。→ git cloneはコピーする命令です。'''\n",
        "!git clone https://github.com/zzh8829/yolov3-tf2.git ./yolov3_tf2\n",
        "\n",
        "''' 今の作業場所を./yolov3_tf2フォルダに移動します。→ 例えば、机の上にある「yolov3_tf2」箱の中を見るイメージ。'''\n",
        "%cd ./yolov3_tf2\n",
        "\n",
        "''' コピーした中のソフトウェアの状態を「c43df87d...」という特定の時点に戻します。\n",
        "→ Gitはバージョン管理ツールで、開発の履歴を管理しています。→ checkoutはその中から特定の時点を選ぶコマンド。'''\n",
        "!git checkout c43df87d8582699aea8e9768b4ebe8d7fe1c6b4c\n",
        "\n",
        "''' ひとつ上のフォルダに戻ります。→ 机の上の箱の外に出るイメージ。'''\n",
        "%cd ../"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path './yolov3_tf2' already exists and is not an empty directory.\n",
            "/content/drive/MyDrive/11章/yolov3_tf2\n",
            "M\tconvert.py\n",
            "HEAD is now at c43df87 Update dataset.py\n",
            "/content/drive/MyDrive/11章\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5BjFMFnn7Za",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf961dcd-31cc-4e23-acf4-49d478a96529"
      },
      "source": [
        "#YOLOの学習済みモデルのダウンロード\n",
        "'''wgetは「ネットからファイルを取ってくる命令」です。YOLOv3-tinyの学習済みの重み（モデル）ファイルのURLです。\n",
        "実行すると、現在のフォルダに yolov3-tiny.weights という名前で保存されます。'''\n",
        "!wget https://pjreddie.com/media/files/yolov3-tiny.weights"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-04 14:19:56--  https://pjreddie.com/media/files/yolov3-tiny.weights\n",
            "Resolving pjreddie.com (pjreddie.com)... 104.21.88.156, 172.67.185.199, 2606:4700:3030::ac43:b9c7, ...\n",
            "Connecting to pjreddie.com (pjreddie.com)|104.21.88.156|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘yolov3-tiny.weights.4’\n",
            "\n",
            "yolov3-tiny.weights     [ <=>                ]   8.88K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2025-06-04 14:19:56 (1.62 MB/s) - ‘yolov3-tiny.weights.4’ saved [9093]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eByPqawwoVc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e6e2951-7a08-4473-bc8d-5a6f51fc99ab",
        "collapsed": true
      },
      "source": [
        "#ダウンロードしたYOLOの学習済みモデルをKerasから利用出来る形に変換\n",
        "'''ダウンロードしたYOLOの学習済みモデル（yolov3-tiny.weights）を、TensorFlow/Kerasで使いやすい形（.tfファイル）に変換する処理です。'''\n",
        "!python ./yolov3_tf2/convert.py --weights ./yolov3-tiny.weights --output  ./yolov3_tf2/checkpoints/yolov3-tiny. --tiny"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-04 14:19:59.538754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1749046799.637618    1187 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1749046799.670360    1187 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "I0000 00:00:1749046809.207355    1187 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n",
            "\u001b[1mModel: \"yolov3_tiny\"\u001b[0m\n",
            "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
            "│ input (\u001b[94mInputLayer\u001b[0m)  │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,      │          \u001b[32m0\u001b[0m │ -                 │\n",
            "│                     │ \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m)          │            │                   │\n",
            "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
            "│ yolo_darknet        │ [(\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,     │  \u001b[32m6,298,480\u001b[0m │ input[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]       │\n",
            "│ (\u001b[94mFunctional\u001b[0m)        │ \u001b[96mNone\u001b[0m, \u001b[32m256\u001b[0m),       │            │                   │\n",
            "│                     │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,      │            │                   │\n",
            "│                     │ \u001b[96mNone\u001b[0m, \u001b[32m1024\u001b[0m)]      │            │                   │\n",
            "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
            "│ yolo_conv_0         │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,      │    \u001b[32m263,168\u001b[0m │ yolo_darknet[\u001b[32m0\u001b[0m][\u001b[32m…\u001b[0m │\n",
            "│ (\u001b[94mFunctional\u001b[0m)        │ \u001b[96mNone\u001b[0m, \u001b[32m256\u001b[0m)        │            │                   │\n",
            "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
            "│ yolo_conv_1         │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,      │     \u001b[32m33,280\u001b[0m │ yolo_conv_0[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m… │\n",
            "│ (\u001b[94mFunctional\u001b[0m)        │ \u001b[96mNone\u001b[0m, \u001b[32m384\u001b[0m)        │            │ yolo_darknet[\u001b[32m0\u001b[0m][\u001b[32m…\u001b[0m │\n",
            "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
            "│ yolo_output_0       │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,      │  \u001b[32m1,312,511\u001b[0m │ yolo_conv_0[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m] │\n",
            "│ (\u001b[94mFunctional\u001b[0m)        │ \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m85\u001b[0m)      │            │                   │\n",
            "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
            "│ yolo_output_1       │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,      │    \u001b[32m951,295\u001b[0m │ yolo_conv_1[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m] │\n",
            "│ (\u001b[94mFunctional\u001b[0m)        │ \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m85\u001b[0m)      │            │                   │\n",
            "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
            "│ yolo_boxes_0        │ [(\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,     │          \u001b[32m0\u001b[0m │ yolo_output_0[\u001b[32m0\u001b[0m]… │\n",
            "│ (\u001b[94mLambda\u001b[0m)            │ \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m4\u001b[0m),      │            │                   │\n",
            "│                     │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,      │            │                   │\n",
            "│                     │ \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m1\u001b[0m),      │            │                   │\n",
            "│                     │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,      │            │                   │\n",
            "│                     │ \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m80\u001b[0m),     │            │                   │\n",
            "│                     │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,      │            │                   │\n",
            "│                     │ \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m4\u001b[0m)]      │            │                   │\n",
            "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
            "│ yolo_boxes_1        │ [(\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,     │          \u001b[32m0\u001b[0m │ yolo_output_1[\u001b[32m0\u001b[0m]… │\n",
            "│ (\u001b[94mLambda\u001b[0m)            │ \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m4\u001b[0m),      │            │                   │\n",
            "│                     │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,      │            │                   │\n",
            "│                     │ \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m1\u001b[0m),      │            │                   │\n",
            "│                     │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,      │            │                   │\n",
            "│                     │ \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m80\u001b[0m),     │            │                   │\n",
            "│                     │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,      │            │                   │\n",
            "│                     │ \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m4\u001b[0m)]      │            │                   │\n",
            "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
            "│ yolo_nms (\u001b[94mLambda\u001b[0m)   │ [(\u001b[96mNone\u001b[0m, \u001b[32m100\u001b[0m, \u001b[32m4\u001b[0m),  │          \u001b[32m0\u001b[0m │ yolo_boxes_0[\u001b[32m0\u001b[0m][\u001b[32m…\u001b[0m │\n",
            "│                     │ (\u001b[96mNone\u001b[0m, \u001b[32m100\u001b[0m),      │            │ yolo_boxes_0[\u001b[32m0\u001b[0m][\u001b[32m…\u001b[0m │\n",
            "│                     │ (\u001b[96mNone\u001b[0m, \u001b[32m100\u001b[0m),      │            │ yolo_boxes_0[\u001b[32m0\u001b[0m][\u001b[32m…\u001b[0m │\n",
            "│                     │ (\u001b[96mNone\u001b[0m)]           │            │ yolo_boxes_1[\u001b[32m0\u001b[0m][\u001b[32m…\u001b[0m │\n",
            "│                     │                   │            │ yolo_boxes_1[\u001b[32m0\u001b[0m][\u001b[32m…\u001b[0m │\n",
            "│                     │                   │            │ yolo_boxes_1[\u001b[32m0\u001b[0m][\u001b[32m…\u001b[0m │\n",
            "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
            "\u001b[1m Total params: \u001b[0m\u001b[32m8,858,734\u001b[0m (33.79 MB)\n",
            "\u001b[1m Trainable params: \u001b[0m\u001b[32m8,852,366\u001b[0m (33.77 MB)\n",
            "\u001b[1m Non-trainable params: \u001b[0m\u001b[32m6,368\u001b[0m (24.88 KB)\n",
            "I0604 14:20:11.122818 134453834592256 convert.py:24] model created\n",
            "I0604 14:20:11.365621 134453834592256 utils.py:44] yolo_darknet/conv2d bn\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/11章/./yolov3_tf2/convert.py\", line 39, in <module>\n",
            "    app.run(main)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/absl/app.py\", line 308, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/absl/app.py\", line 254, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "             ^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/11章/./yolov3_tf2/convert.py\", line 26, in main\n",
            "    load_darknet_weights(yolo, FLAGS.weights, FLAGS.tiny)\n",
            "  File \"/content/drive/MyDrive/11章/yolov3_tf2/yolov3_tf2/utils.py\", line 49, in load_darknet_weights\n",
            "    in_dim = layer.get_input_shape_at(0)[-1]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'Conv2D' object has no attribute 'get_input_shape_at'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### YOLOによる物体検出の実行"
      ],
      "metadata": {
        "id": "DKPYmAp-wvtJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Hp9rZ5q1Iay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59a1f654-6dc0-4e01-d3e5-2415d195cc0d"
      },
      "source": [
        "from absl import app, logging, flags\n",
        "from absl.flags import FLAGS\n",
        "'''プログラムを動かす前に「このプログラムはyolov3という名前で動きますよ」「コマンドラインで渡されたオプションを読み込みますよ」という準備をしています。'''\n",
        "app._run_init(['yolov3'], app.parse_flags_with_usage)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['yolov3']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2bBDwf61Ip8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aa32b119-9136-4969-8e3a-d0f78f493bf9"
      },
      "source": [
        "#学習済みの重みをそのまま利用する場合\n",
        "from  yolov3_tf2.yolov3_tf2.models import  YoloV3Tiny, YoloLoss\n",
        "import tensorflow as tf\n",
        "from yolov3_tf2.yolov3_tf2.dataset import transform_images\n",
        "from yolov3_tf2.yolov3_tf2.utils import draw_outputs\n",
        "import numpy as np\n",
        "\n",
        "!python ./yolov3_tf2/convert.py --weights ./yolov3-tiny.weights --output ./yolov3_tf2/checkpoints/yolov3-tiny --tiny\n",
        "\n",
        "FLAGS.yolo_iou_threshold = 0.5\n",
        "FLAGS.yolo_score_threshold = 0.5\n",
        "\n",
        "yolo_class_names = [c.strip() for c in open(\"./yolov3_tf2/data/coco.names\").readlines()]\n",
        "\n",
        "yolo = YoloV3Tiny(classes=80)\n",
        "#重みの読み込み\n",
        "yolo.load_weights(\"./yolov3_tf2/checkpoints/yolov3-tiny.h5\").expect_partial()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-04 14:57:02.064562: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1749049022.085053   10615 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1749049022.090961   10615 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "I0000 00:00:1749049026.054605   10615 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13714 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n",
            "\u001b[1mModel: \"yolov3_tiny\"\u001b[0m\n",
            "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
            "│ input (\u001b[94mInputLayer\u001b[0m)  │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,      │          \u001b[32m0\u001b[0m │ -                 │\n",
            "│                     │ \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m)          │            │                   │\n",
            "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
            "│ yolo_darknet        │ [(\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,     │  \u001b[32m6,298,480\u001b[0m │ input[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m]       │\n",
            "│ (\u001b[94mFunctional\u001b[0m)        │ \u001b[96mNone\u001b[0m, \u001b[32m256\u001b[0m),       │            │                   │\n",
            "│                     │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,      │            │                   │\n",
            "│                     │ \u001b[96mNone\u001b[0m, \u001b[32m1024\u001b[0m)]      │            │                   │\n",
            "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
            "│ yolo_conv_0         │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,      │    \u001b[32m263,168\u001b[0m │ yolo_darknet[\u001b[32m0\u001b[0m][\u001b[32m…\u001b[0m │\n",
            "│ (\u001b[94mFunctional\u001b[0m)        │ \u001b[96mNone\u001b[0m, \u001b[32m256\u001b[0m)        │            │                   │\n",
            "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
            "│ yolo_conv_1         │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,      │     \u001b[32m33,280\u001b[0m │ yolo_conv_0[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m… │\n",
            "│ (\u001b[94mFunctional\u001b[0m)        │ \u001b[96mNone\u001b[0m, \u001b[32m384\u001b[0m)        │            │ yolo_darknet[\u001b[32m0\u001b[0m][\u001b[32m…\u001b[0m │\n",
            "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
            "│ yolo_output_0       │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,      │  \u001b[32m1,312,511\u001b[0m │ yolo_conv_0[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m] │\n",
            "│ (\u001b[94mFunctional\u001b[0m)        │ \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m85\u001b[0m)      │            │                   │\n",
            "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
            "│ yolo_output_1       │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,      │    \u001b[32m951,295\u001b[0m │ yolo_conv_1[\u001b[32m0\u001b[0m][\u001b[32m0\u001b[0m] │\n",
            "│ (\u001b[94mFunctional\u001b[0m)        │ \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m85\u001b[0m)      │            │                   │\n",
            "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
            "│ yolo_boxes_0        │ [(\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,     │          \u001b[32m0\u001b[0m │ yolo_output_0[\u001b[32m0\u001b[0m]… │\n",
            "│ (\u001b[94mLambda\u001b[0m)            │ \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m4\u001b[0m),      │            │                   │\n",
            "│                     │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,      │            │                   │\n",
            "│                     │ \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m1\u001b[0m),      │            │                   │\n",
            "│                     │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,      │            │                   │\n",
            "│                     │ \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m80\u001b[0m),     │            │                   │\n",
            "│                     │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,      │            │                   │\n",
            "│                     │ \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m4\u001b[0m)]      │            │                   │\n",
            "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
            "│ yolo_boxes_1        │ [(\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,     │          \u001b[32m0\u001b[0m │ yolo_output_1[\u001b[32m0\u001b[0m]… │\n",
            "│ (\u001b[94mLambda\u001b[0m)            │ \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m4\u001b[0m),      │            │                   │\n",
            "│                     │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,      │            │                   │\n",
            "│                     │ \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m1\u001b[0m),      │            │                   │\n",
            "│                     │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,      │            │                   │\n",
            "│                     │ \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m80\u001b[0m),     │            │                   │\n",
            "│                     │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m,      │            │                   │\n",
            "│                     │ \u001b[96mNone\u001b[0m, \u001b[32m3\u001b[0m, \u001b[32m4\u001b[0m)]      │            │                   │\n",
            "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
            "│ yolo_nms (\u001b[94mLambda\u001b[0m)   │ [(\u001b[96mNone\u001b[0m, \u001b[32m100\u001b[0m, \u001b[32m4\u001b[0m),  │          \u001b[32m0\u001b[0m │ yolo_boxes_0[\u001b[32m0\u001b[0m][\u001b[32m…\u001b[0m │\n",
            "│                     │ (\u001b[96mNone\u001b[0m, \u001b[32m100\u001b[0m),      │            │ yolo_boxes_0[\u001b[32m0\u001b[0m][\u001b[32m…\u001b[0m │\n",
            "│                     │ (\u001b[96mNone\u001b[0m, \u001b[32m100\u001b[0m),      │            │ yolo_boxes_0[\u001b[32m0\u001b[0m][\u001b[32m…\u001b[0m │\n",
            "│                     │ (\u001b[96mNone\u001b[0m)]           │            │ yolo_boxes_1[\u001b[32m0\u001b[0m][\u001b[32m…\u001b[0m │\n",
            "│                     │                   │            │ yolo_boxes_1[\u001b[32m0\u001b[0m][\u001b[32m…\u001b[0m │\n",
            "│                     │                   │            │ yolo_boxes_1[\u001b[32m0\u001b[0m][\u001b[32m…\u001b[0m │\n",
            "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
            "\u001b[1m Total params: \u001b[0m\u001b[32m8,858,734\u001b[0m (33.79 MB)\n",
            "\u001b[1m Trainable params: \u001b[0m\u001b[32m8,852,366\u001b[0m (33.77 MB)\n",
            "\u001b[1m Non-trainable params: \u001b[0m\u001b[32m6,368\u001b[0m (24.88 KB)\n",
            "I0604 14:57:07.229207 137097035202560 convert.py:24] model created\n",
            "I0604 14:57:07.231979 137097035202560 utils.py:44] yolo_darknet/conv2d bn\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/11章/./yolov3_tf2/convert.py\", line 39, in <module>\n",
            "    app.run(main)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/absl/app.py\", line 308, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/absl/app.py\", line 254, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "             ^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/11章/./yolov3_tf2/convert.py\", line 26, in main\n",
            "    load_darknet_weights(yolo, FLAGS.weights, FLAGS.tiny)\n",
            "  File \"/content/drive/MyDrive/11章/yolov3_tf2/yolov3_tf2/utils.py\", line 49, in load_darknet_weights\n",
            "    in_dim = layer.get_input_shape_at(0)[-1]\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'Conv2D' object has no attribute 'get_input_shape_at'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = './yolov3_tf2/checkpoints/yolov3-tiny.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-10a5525fd55d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0myolo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYoloV3Tiny\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#重みの読み込み\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0myolo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./yolov3_tf2/checkpoints/yolov3-tiny.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_partial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                  \u001b[0mfs_persist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_persist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfs_threshold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                                  fs_page_size=fs_page_size)\n\u001b[0;32m--> 564\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = './yolov3_tf2/checkpoints/yolov3-tiny.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0_NTn0P_VWC"
      },
      "source": [
        "img_filename = \"img/img01.jpg\"\n",
        "img_rawP = tf.image.decode_jpeg(open(img_filename, 'rb').read(), channels=3)\n",
        "data_shape=(256,256,3)\n",
        "img_yoloP = transform_images(img_rawP, data_shape[0])\n",
        "img_yoloP = np.expand_dims(img_yoloP, 0)\n",
        "# 予測開始\n",
        "boxes, scores, classes, nums = yolo.predict(img_yoloP)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 結果の出力"
      ],
      "metadata": {
        "id": "ZS5abmZ7xBQ_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpFo8yPt_dte"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img_yoloP = img_rawP.numpy()\n",
        "img_yoloP = draw_outputs(img_yoloP, (boxes, scores, classes, nums), yolo_class_names)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(img_yoloP)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 放課後ノック106：YOLOの学習を行うための準備をしよう"
      ],
      "metadata": {
        "id": "WFl2r6FDt_P1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習データのダウンロード"
      ],
      "metadata": {
        "id": "XCWodp_mxrvG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_sBaX_5nJBz",
        "collapsed": true
      },
      "source": [
        "#データセットのダウンロード及び解凍を行います。\n",
        "#ダウンロード済みでない場合以下を実行して下さい。\n",
        "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
        "!tar -xvf VOCtrainval_06-Nov-2007.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習データの確認"
      ],
      "metadata": {
        "id": "zim5JC_jxviJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRSb05V_ryGC"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "#ダウンロードしたデータセットの画像の内１枚を表示\n",
        "Image.open(\"./VOCdevkit/VOC2007/JPEGImages/006626.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7-aAtnWryIi"
      },
      "source": [
        "#表示した画像のアノテーションデータの表示\n",
        "annotation = open(\"./VOCdevkit/VOC2007/Annotations/006626.xml\").read()\n",
        "print(annotation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 放課後ノック107：新たな学習データを使ってYOLOの学習モデルを生成してみよう"
      ],
      "metadata": {
        "id": "8WeCtG4VxkEZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ライブラリのインストール"
      ],
      "metadata": {
        "id": "scd8ku8ix3-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xmltodict"
      ],
      "metadata": {
        "id": "19AfE-oBoywW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習データの変換"
      ],
      "metadata": {
        "id": "t2E8AJvSx7a8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLnLblWMqm2p"
      },
      "source": [
        "import xmltodict\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import math\n",
        "import yolov3_tf2.yolov3_tf2.dataset as dataset\n",
        "\n",
        "yolo_max_boxes = 100\n",
        "\n",
        "#アノテーションデータの変換\n",
        "def parse_annotation(annotation, class_map):\n",
        "    label = []\n",
        "    width = int(annotation['size']['width'])\n",
        "    height = int(annotation['size']['height'])\n",
        "\n",
        "    if 'object' in annotation:\n",
        "        if type(annotation['object']) != list:\n",
        "            tmp = [annotation['object']]\n",
        "        else:\n",
        "            tmp = annotation['object']\n",
        "\n",
        "        for obj in tmp:\n",
        "            _tmp = []\n",
        "            _tmp.append(float(obj['bndbox']['xmin']) / width)\n",
        "            _tmp.append(float(obj['bndbox']['ymin']) / height)\n",
        "            _tmp.append(float(obj['bndbox']['xmax']) / width)\n",
        "            _tmp.append(float(obj['bndbox']['ymax']) / height)\n",
        "            _tmp.append(class_map[obj['name']])\n",
        "            label.append(_tmp)\n",
        "\n",
        "    for _ in range(yolo_max_boxes - len(label)):\n",
        "      label.append([0,0,0,0,0])\n",
        "    return label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習データ読み込みクラスの定義"
      ],
      "metadata": {
        "id": "F-xxvU37yPJZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlyQ5j_PoXPa"
      },
      "source": [
        "from yolov3_tf2.yolov3_tf2.dataset import transform_images\n",
        "\n",
        "#学習時に画像データを必要な分だけ読み込むためのクラス\n",
        "class ImageDataSequence(Sequence):\n",
        "    def __init__(self, file_name_list, batch_size,  anchors, anchor_masks, class_names, data_shape=(256,256,3)):\n",
        "\n",
        "        #クラス名とそれに対応する数値、という形の辞書を作る\n",
        "        self.class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "        self.file_name_list = file_name_list\n",
        "\n",
        "        self.image_file_name_list = [\"./VOCdevkit/VOC2007/JPEGImages/\"+image_path + \".jpg\" for image_path in self.file_name_list]\n",
        "        self.annotation_file_name_list = ['./VOCdevkit/VOC2007/Annotations/' + image_path+ \".xml\" for image_path in self.file_name_list]\n",
        "\n",
        "        self.length = len(self.file_name_list)\n",
        "        self.data_shape = data_shape\n",
        "        self.batch_size = batch_size\n",
        "        self.anchors = anchors\n",
        "        self.anchor_masks = anchor_masks\n",
        "\n",
        "        self.labels_cache = [None for i in range(self.__len__())]\n",
        "\n",
        "    #１バッチごとに自動的に呼ばれる。画像データとそのラベルを必要な分だけ読み込んで返す\n",
        "    def __getitem__(self, idx):\n",
        "        images = []\n",
        "        labels = []\n",
        "\n",
        "        #現在のバッチが何回目か、がidx変数に入っているため、それに対応するデータを読み込む\n",
        "        for index in range(idx*self.batch_size, (idx+1)*self.batch_size):\n",
        "\n",
        "          #アノテーションデータをラベルとして使える形に変換する\n",
        "          annotation = xmltodict.parse((open(self.annotation_file_name_list[index]).read()))\n",
        "          label = parse_annotation(annotation[\"annotation\"], self.class_map)\n",
        "          labels.append(label)\n",
        "\n",
        "          #画像データの読み込みと加工\n",
        "          img_raw = tf.image.decode_jpeg(open(self.image_file_name_list[index], 'rb').read(), channels=3)\n",
        "          img = transform_images(img_raw, self.data_shape[0])\n",
        "          images.append(img)\n",
        "\n",
        "        #ラベルに対しても前処理をするが、時間がかかるため１度読み込んだらキャッシュとして保存する\n",
        "        if self.labels_cache[idx] is None:\n",
        "          labels = tf.convert_to_tensor(labels, tf.float32)\n",
        "          labels = dataset.transform_targets(labels, self.anchors, self.anchor_masks, self.data_shape[0])\n",
        "          self.labels_cache[idx] = labels\n",
        "        else:\n",
        "          labels = self.labels_cache[idx]\n",
        "\n",
        "        images = np.array(images)\n",
        "        return images, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.floor(len(self.file_name_list) / self.batch_size)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### YOLOモデル(ネットワーク)の読み込み"
      ],
      "metadata": {
        "id": "FmImgsydy7QY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "print(keras.__version__)\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.keras.__version__)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pxJPOHHj1GkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT83GAyExapw"
      },
      "source": [
        "from  yolov3_tf2.yolov3_tf2.models import  YoloV3Tiny, YoloLoss\n",
        "from yolov3_tf2.yolov3_tf2.utils import freeze_all\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size=16\n",
        "data_shape=(416,416,3)\n",
        "class_names =  [\"person\", \"bird\", \"cat\",\"cow\",\"dog\", \"horse\",\"sheep\", \"aeroplane\", \"bicycle\", \"boat\", \"bus\", \"car\", \"motorbike\", \"train\", \"bottle\", \"chair\", \"diningtable\", \"pottedplant\", \"sofa\", \"tvmonitor\"]\n",
        "\n",
        "anchors = np.array([(10, 14), (23, 27), (37, 58),\n",
        "                              (81, 82), (135, 169),  (344, 319)],\n",
        "                             np.float32) / data_shape[0]\n",
        "anchor_masks = np.array([[3, 4, 5], [0, 1, 2]])\n",
        "\n",
        "# yolov3_tf2で定義されているtiny YOLOのモデルを読み込む\n",
        "model_pretrained = YoloV3Tiny(data_shape[0], training=True, classes=80)\n",
        "model_pretrained.load_weights(\"./yolov3_tf2/checkpoints/yolov3-tiny.tf\").expect_partial()\n",
        "\n",
        "model = YoloV3Tiny(data_shape[0], training=True, classes=len(class_names))\n",
        "#ここで、学習済みモデルの出力層以外の重みだけを取り出す\n",
        "model.get_layer('yolo_darknet').set_weights(model_pretrained.get_layer('yolo_darknet').get_weights())\n",
        "#出力層以外を学習しないようにする\n",
        "freeze_all(model.get_layer('yolo_darknet'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uv8wOy-yJfO"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.keras.__version__)\n",
        "loss = [YoloLoss(anchors[mask], classes=len(class_names)) for mask in anchor_masks]\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001), loss=loss, run_eagerly=False)\n",
        "\n",
        "#モデルの構造を出力\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習データの読み込み"
      ],
      "metadata": {
        "id": "UySDjHGW0BHf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5xgUKjiyRSQ"
      },
      "source": [
        "train_file_name_list = open(\"./VOCdevkit/VOC2007/ImageSets/Main/train.txt\").read().splitlines()\n",
        "validation_file_name_list = open(\"./VOCdevkit/VOC2007/ImageSets/Main/val.txt\").read().splitlines()\n",
        "\n",
        "train_dataset = ImageDataSequence(train_file_name_list, batch_size, anchors, anchor_masks, class_names, data_shape=data_shape)\n",
        "validation_dataset = ImageDataSequence(validation_file_name_list, batch_size, anchors, anchor_masks, class_names, data_shape=data_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習の実施"
      ],
      "metadata": {
        "id": "PiVSTcSr0ErM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQrv-F_YyW6f"
      },
      "source": [
        "history = model.fit(train_dataset, validation_data=validation_dataset, epochs=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHdcPhz_yZXU"
      },
      "source": [
        "#学習した重みの保存\n",
        "model.save_weights('./saved_models/model_yolo_weights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OqkGbOW3c6i"
      },
      "source": [
        "### 放課後ノック108：新たに学習させたモデルを使って人の検出を行ってみよう"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習した重みの読み込み"
      ],
      "metadata": {
        "id": "cK8VHMiO0zr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from absl import app, logging, flags\n",
        "from absl.flags import FLAGS\n",
        "app._run_init(['yolov3'], app.parse_flags_with_usage)"
      ],
      "metadata": {
        "id": "1j2EScKY12gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XizpUkI11IdK"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from yolov3_tf2.yolov3_tf2.utils import draw_outputs\n",
        "\n",
        "FLAGS.yolo_iou_threshold = 0.5\n",
        "FLAGS.yolo_score_threshold = 0.5\n",
        "\n",
        "yolo_trained = YoloV3Tiny(classes=len(class_names))\n",
        "#保存した重みの読み込み\n",
        "yolo_trained.load_weights('./saved_models/model_yolo_weights').expect_partial()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 物体検出の実行"
      ],
      "metadata": {
        "id": "IdcccUPa03cN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smr62Jkw1Ifq"
      },
      "source": [
        "img_filename = \"img/img01.jpg\"\n",
        "\n",
        "#画像の読み込み\n",
        "img_rawL = tf.image.decode_jpeg(open(img_filename, 'rb').read(), channels=3)\n",
        "img_yoloL = transform_images(img_rawL, data_shape[0])\n",
        "img_yoloL = np.expand_dims(img_yoloL, 0)\n",
        "\n",
        "\n",
        "#予測開始\n",
        "boxes, scores, classes, nums = yolo_trained.predict(img_yoloL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 結果の表示"
      ],
      "metadata": {
        "id": "aDmfgGLI1FkX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKiCvBwC1Ih4"
      },
      "source": [
        "img_yoloL = img_rawL.numpy()\n",
        "\n",
        "#予測結果を画像に書き込み\n",
        "img_yoloL = draw_outputs(img_yoloL, (boxes, scores, classes, nums), class_names)\n",
        "\n",
        "#予測結果を書き込んだ画像の表示\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(img_yoloL)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 放課後ノック109：YOLOとHOGの人の検出結果を比較して深層学習の精度を体感しよう"
      ],
      "metadata": {
        "id": "KMterp_J1cgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HOGによる人の検出"
      ],
      "metadata": {
        "id": "44hQiGY623G_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# 準備 #\n",
        "hog = cv2.HOGDescriptor()\n",
        "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
        "hogParams = {'winStride': (8, 8), 'padding': (32, 32), 'scale': 1.05, 'hitThreshold':0, 'groupThreshold':5}\n",
        "\n",
        "# 検出 #\n",
        "img_hog = cv2.imread(\"img/img01.jpg\")\n",
        "gray = cv2.cvtColor(img_hog, cv2.COLOR_BGR2GRAY)\n",
        "human, r = hog.detectMultiScale(gray, **hogParams)\n",
        "if (len(human)>0):\n",
        "    for (x, y, w, h) in human:\n",
        "        cv2.rectangle(img_hog, (x, y), (x + w, y + h), (255,255,255), 3)\n",
        "\n",
        "# 表示 #\n",
        "img_hog = cv2.cvtColor(img_hog, cv2.COLOR_BGR2RGB)\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(img_hog)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FKujwzGC1g1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 結果の比較"
      ],
      "metadata": {
        "id": "8hsy8O-u5t6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(30,10))\n",
        "plt.subplot(1,3,1)\n",
        "plt.title(\"YOLO (pre-trained)\")\n",
        "plt.imshow(img_yoloP)\n",
        "plt.axis('off')\n",
        "plt.subplot(1,3,2)\n",
        "plt.title(\"YOLO (trained)\")\n",
        "plt.imshow(img_yoloL)\n",
        "plt.axis('off')\n",
        "plt.subplot(1,3,3)\n",
        "plt.title(\"HOG\")\n",
        "plt.imshow(img_hog)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RIQp-9Oq1g8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 放課後ノック110：YOLOでの人以外の物体の検出のようすを確認しよう"
      ],
      "metadata": {
        "id": "wGdqlEqWNUYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_filename = \"img/img02.jpg\"\n",
        "\n",
        "#画像の読み込み\n",
        "img_rawP = tf.image.decode_jpeg(open(img_filename, 'rb').read(), channels=3)\n",
        "img_yoloP = transform_images(img_rawP, data_shape[0])\n",
        "img_yoloP = np.expand_dims(img_yoloP, 0)\n",
        "\n",
        "#クラス名の読み込み\n",
        "yolo_class_names = [c.strip() for c in open(\"./yolov3_tf2/data/coco.names\").readlines()]\n",
        "\n",
        "#予測開始\n",
        "boxes, scores, classes, nums = yolo.predict(img_yoloP)\n",
        "\n",
        "#予測結果を画像に書き込み\n",
        "img_yoloP = img_rawP.numpy()\n",
        "img_yoloP = draw_outputs(img_yoloP, (boxes, scores, classes, nums), yolo_class_names)\n",
        "\n",
        "#予測結果を書き込んだ画像の表示\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(img_yoloP)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X2lcXcRHnLI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "collapsed": true,
        "id": "OmVe83TRNlSF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}